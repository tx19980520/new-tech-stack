# 使用nginx搭建基于IP封禁的反向代理服务

由于业务需要，我们需要提供用户真实IP来进行相应的封禁，我们的初步想法是在整个集群的ingress外层进行相应的配置，具体的封禁策略与内部的服务集群不进行强绑定，进行统一的集中式的管理。

nginx做IP层级的封禁的主要子任务是访问次数统计和黑名单存储读写的相关问题。

## 访问次数的统计

主要是基于nginx里的`remote_addr`来进行计数，我们的计数情况存储在在nginx的内存中，规定一定大小内存（`lua_shared_dict`）当做字典进行存放，该空间是进程间进行共享的（这是必须的）。

## 黑名单存储

我们最初的版本是使用仍旧使用`lua_shared_dict`进行进行次数的比较。

## 进阶问题

### 存储问题

我们预想的是统计次数是一个非常散的操作，是需要进行比较及时的剔除，因为有的用户的访问仅仅就是一次两次的正常访问，我们肯定是相信大部分的用户是正常操作的。那这样就有可能可以通过hack的方式来躲过检查，我们的`lua_shared_dict`这块空间也是相对会随着用户数量的提升从而正比进行提升。

在此的一个利好消息是，`lua_shared_dict`的在空间不够的情况下是使用最近最少使用的条目来被覆盖，当然也不需要考虑到说updated_time会被清理掉（在我们的nil处理下即便是被清理掉了也不会有太大的问题。）

我们会考虑使用redis存储计数，然后再内存中只存储黑名单即可。

### 粒度问题

我们拦截的单位现在是整个系统为单位，我们的细粒度的控制（某个微服务）现阶段的想法是直接写多个location在本层次进行控制，对于不同的服务我们使用不同的黑名单，或者是部分共用的黑名单（例如超过100次是某一微服务封禁，但是如果总共超过1000次进入到全局黑名单）。

### 前端文件的缓存

前端仅仅就是一个文件，我们部署前端是直接使用nginx+前端文件进行打包，我们这里相当于多了一层代理，性能上可能有一点点多余的消耗，因此首先是要考虑这种静态文件如何被访问到，另一方面要考虑到访问的性能如何，会不会在更新了前端的Pod之后，出现代理的pod没有及时的更新缓存，导致部署出现延迟这样的情况。

## 基于IP封禁的性能测试

给予IP封禁，对于用户而言，定然是会加大延迟，必须设置在能够接受的范围内。我们使用JMeter进行性能测试，来进行初步的探究。

### 模仿正常用户进行访问

#### 低压正常用户访问

#### 高压正常用户访问

### 爬虫访问

